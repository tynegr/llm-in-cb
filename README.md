# УП "Применение LLM" 
*Тищенко Егор и Арзамазов Никита*

## Содержание проекта 
1. RAG система
2. LLM
3. Чат-бот в телеграм
4. Парсер

## Как все устроено?

**0.** Парсер собирает данные по телеграмм каналам из файла ```llm_in_cb/parser/tg_channels.txt``` и заливает в RAG с помощью контейнера ```vector_api```.
**1.** Пользователь заходит в чат-бота (контейнер ```telegram_bot```), через контекстное меню выбирает модель и делает запрос. 
**2.** Формируется эмбеддинг запроса и к нему в RAG системе ищется подходящий контекст по мере близости векторов (обращение к ```vector_api```). 
**3.** Запрос и контекст конкатенируются в промт и подаются ранее выбранной LLM (обращение к ```llama_cpp``` или ```llama_cpp```, зависит от выбора пользователя). 
**4.** Ответ возвращается в чат пользователю.

## Ограничения и недоделки

**1.** Мы не смогли настройить парсер с кроном, пока что он собирает все единоразово, поэтому контекст может быстро устаревать
**2.** Из-за ограничений гитхаба на размер заливаемых файлов и квоты в **git-lfs** получилось залить только одну LLM.

## Запуск проекта

Запуск проекта осуществляется одной простой командой:

```bash
docker compose up --build
```

## Примечание

github накладывает ограничения на общий размер файлов в репозитории. В связи с этим удалось загрузить только одну небольшую модель. 

Тем не менее, архитектура решения позволяет легко интегрировать любую модель в формате **gguf**. Для этого потребуется изменить всего две строки кода в docker-compose.yml.
