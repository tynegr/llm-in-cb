# УП "Применение LLM" 
*Тищенко Егор и Арзамазов Никита*

## Содержание проекта 
1. RAG система
2. LLM (Qwen2.5-Coder-1.5B)
3. Чат-бот в телеграм **@CentralBankChatbot**
4. Парсер
5. Модель для эмбеддингов (Multilingual-E5-small)

## Как все устроено?

0. Парсер собирает данные по телеграмм каналам из файла ```llm_in_cb/parser/tg_channels.txt``` и заливает в RAG с помощью контейнера ```vector_api```.
1. Пользователь заходит в чат-бота (контейнер ```telegram_bot```), через контекстное меню выбирает модель и делает запрос. 
2. Формируется эмбеддинг запроса (``` embedding_api ```) и к нему в RAG системе ищется подходящий контекст по мере близости векторов (обращение к ```vector_api```). 
3. Запрос и контекст конкатенируются в промт и подаются ранее выбранной LLM (обращение к ```Qwen2.5-1.5B``` или ```Vikhr-Qwen-2.5-1.5B```, зависит от выбора пользователя). 
4. Ответ возвращается в чат пользователю.

## Ограничения и недоделки

1. Пока что парсер должен запускаться один раз после поднятия контейнеров. В перспективе, можно запускать парсер с кроном, либо обойтись  каким-нибудь search API. 
2. Из-за ограничений гитхаба на размер заливаемых файлов и квоты в **git-lfs** получилось залить только одну LLM.

## Запуск проекта

Запуск проекта осуществляется одной простой командой:

```bash
docker compose up --build
```

## Примечание

github накладывает ограничения на общий размер файлов в репозитории. В связи с этим удалось загрузить только одну небольшую модель. 

Тем не менее, архитектура решения позволяет легко интегрировать любую модель в формате **gguf**. Для этого потребуется изменить всего две строки кода в docker-compose.yml.
