version: "3.9"

services:
  qdrant:
    image: qdrant/qdrant
    container_name: qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage:z

  embedding_api:
    build:
      context: .
    container_name: embedding_api
    environment:
      QDRANT_URL: "http://qdrant:6333"
      EMBED_PORT: "6666"
      EMBED_SIZE: "384"
      MODEL_NAME: "intfloat/multilingual-e5-small"
      COLLECTION_NAME: "llm_in_cb_test_2"
    depends_on:
      - qdrant
    command: [ "poetry", "run", "uvicorn", "llm_in_cb.rag.embedder.embedding_api:app", "--host", "0.0.0.0", "--port", "6666" ]
    ports:
      - "6666:6666"

  vector_api:
    build:
      context: .
    container_name: vector_api
    environment:
      QDRANT_URL: "http://qdrant:6333"
      VECTOR_PORT: "5555"
      EMBED_SIZE: "384"
      COLLECTION_NAME: "llm_in_cb_test_2"
    depends_on:
      - qdrant
    command: [ "poetry", "run", "uvicorn", "llm_in_cb.rag.vector_api:app", "--host", "0.0.0.0", "--port", "5555" ]
    ports:
      - "5555:5555"

#  llama_cpp:
#    build:
#      context: .
#      dockerfile: llm_in_cb/llm_api/Dockerfile.llama
#    container_name: llama_cpp
#    ports:
#      - "8080:8080"
#    volumes:
#      - ./data:/llama.cpp/models
#    environment:
#      MODEL_PATH: "llm_in_cb/llm_api/model_file/qwen2.5-coder-1.5b-q8_0.gguf"
#    command: [ "./examples/server/llama-server", "--host", "0.0.0.0", "--port", "1349", "--hf-file", "/llama.cpp/models/qwen2.5-coder-1.5b-q8_0.gguf" ]

  llama_cpp:
    image: ghcr.io/ggerganov/llama.cpp:server
    container_name: llama_cpp
    ports:
      - "1349:1349"  # Port mapping for the server
    volumes:
      - ./models:/models
    environment:
      MODEL_PATH: "/models/qwen2.5-coder-1.5b-q8_0.gguf"
    command: ["-m", "/models/qwen2.5-coder-1.5b-q8_0.gguf", "--port", "1349", "--host", "0.0.0.0"]



  bot:
    build:
      context: .
    container_name: telegram_bot
    environment:
      TGBOT_API: "7934205115:AAGWUC4oDvzVbUBrKTiThJtjm2J_YAKoT1Q"
      EMBEDDING_API_URL: "http://embedding_api:6666"
      VECTOR_DB_API_URL: "http://vector_api:5555"
      LLM_API_URL: "http://llama_cpp:1349/completion"
    depends_on:
      - embedding_api
      - vector_api
    command: [ "poetry", "run", "python", "llm_in_cb/bot/botik.py" ]

volumes:
  qdrant_data:
