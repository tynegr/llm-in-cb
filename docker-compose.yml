version: "3.9"

services:
  qdrant:
    image: qdrant/qdrant
    container_name: qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage:z

  embedding_api:
    build:
      context: .
    container_name: embedding_api
    environment:
      QDRANT_URL: "http://qdrant:6333"
      EMBED_PORT: "6666"
      EMBED_SIZE: "384"
      MODEL_NAME: "intfloat/multilingual-e5-small"
      COLLECTION_NAME: "llm_in_cb_test_2"
    depends_on:
      - qdrant
    command: [ "poetry", "run", "uvicorn", "llm_in_cb.rag.embedder.embedding_api:app", "--host", "0.0.0.0", "--port", "6666" ]
    ports:
      - "6666:6666"

  vector_api:
    build:
      context: .
    container_name: vector_api
    environment:
      QDRANT_URL: "http://qdrant:6333"
      VECTOR_PORT: "5555"
      EMBED_SIZE: "384"
      COLLECTION_NAME: "llm_in_cb_test_2"
    depends_on:
      - qdrant
    command: [ "poetry", "run", "uvicorn", "llm_in_cb.rag.vector_api:app", "--host", "0.0.0.0", "--port", "5555" ]
    ports:
      - "5555:5555"

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_MODEL=llama3.1:1b
#    command: ["ollama", "serve"]

  bot:
    build:
      context: .
    container_name: telegram_bot
    environment:
      TGBOT_API: "7934205115:AAGWUC4oDvzVbUBrKTiThJtjm2J_YAKoT1Q"
      EMBEDDING_API_URL: "http://embedding_api:6666"
      VECTOR_DB_API_URL: "http://vector_api:5555"
      LLM_API_URL: "http://ollama:11434/api/generate"
    depends_on:
      - embedding_api
      - vector_api
      - ollama
    command: [ "poetry", "run", "python", "llm_in_cb/bot/bot.py" ]

volumes:
  qdrant_data:
  ollama_data:
